<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252" charset="utf-8">
<title>Jinpeng Wang - PHD, National University of Singaoire </title>
    <style>
  body{
  position:relative;
  }
  .img {width: 20px;height: 20px;opacity: 1;position: absolute;z-index: 100000;transition: 2s;}
  .left,.right {width: 10px;height: 10px;border-radius: 100%;position: absolute;}
  .right {right: 0;}
  .under {width: 10px;height: 10px;position: absolute;top: 5px;left: 5px;transform: rotate(45deg)}
  .text {width: 50px;font-size: 10px;line-height: 1;position: absolute;top: -1em;left: -15px;text-align: center;}

 </style>

 </head>
<body>
<div class="mydiv"></div>
</body>
<script>
      text = ["Hello", "Hi"];
      // count
      var count = 0;
      // click event
      document.body.onmousedown = function (e) {
          // pos
          var x = event.pageX - 18;
         var y = event.pageY - 30;
        // element
        var img = document.createElement("div");
         var left = document.createElement("div");
         var right = document.createElement("div");
         var under = document.createElement("div");
         var txt = document.createElement("div");
         // txt
        var textNode = document.createTextNode(text[parseInt(Math.random() * text.length)]);
         // txt
        txt.appendChild(textNode);

         img.className = "img" + " " + "img" + count;
         left.className = "left";
         right.className = "right";
         under.className = "under";
        txt.className = "text";
         img.style.top = y + "px";
        img.style.left = x + "px";
        img.appendChild(left);
         img.appendChild(right);
        img.appendChild(under);
       img.appendChild(txt);
         document.body.appendChild(img);
         // random color
         var color = "rgb(" + parseInt(Math.random() * 255) + "," + parseInt(Math.random() * 255) + "," +
             parseInt(Math.random() * 255) + ")";
         txt.style.color=color;
         for (var i = 0; i < 3; i++) {
             img.children[i].style.background = color;
        }
     }
     document.body.onmouseup = function () {
         document.getElementsByClassName("img" + count)[0].style.transform = "scale(0.5)";
         document.getElementsByClassName("img" + count)[0].style.transform = "translateY(-40px)";
         document.getElementsByClassName("img" + count)[0].style.opacity = "0";
         count++;
    }
 </script>
<style type="text/css"></style></head>
    <body><table border="0" width="980px" align="center"><tbody><tr><td>
    
        </td><td valign="top">
<!--            <img src="images/cmuscslogo.gif">
                <img src="images/rilogo.png"> -->
        <br>
        <table style="font-size: 11pt;" border="0" width="100%">
            <tbody><tr>
                <td width="50%">
                    <img width="230" src="./index_files/jinpeng/my_photo_2.jpg" border="0" style="border-radius: 50%;overflow: hidden">
                </td>
                <td>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="6"> 
                        <b> [Alex] Jinpeng Wang (ÁéãÈáëÈπè)</b><br><br>
                    </font>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="4"> 
                        Tenure-track Professor, Independent PI<br>
                        Central South University (CSU)<br>
                        China<br><br>
                        jinpengwang at u.nus.edu<br>
                        [<a href="https://github.com/fingerrec" border="0">GitHub</a>]
                        [<a href="https://scholar.google.com/citations?user=UtaAVacAAAAJ&hl=zh-CN" border="0">Google Scholar</a>]<br>
                        [<a href="https://www.semanticscholar.org/author/Jinpeng-Wang/48093158" border="0">Semantic Scholar</a>]<br>
                        [<a href="https://twitter.com/awinyimgprocess" border="0">Twitter</a>]
                        [<a href="bio.txt" border="0">Bio</a>]<br>
                    </font>
                </td>
            </tr>
        </tbody></table> 
        <p>
        </p><hr size="2" align="left" noshade="">
        <p>
        
        <font face="helvetica, ariel, &#39;sans serif&#39;"> 
        <!--</font></p><h2><font face="helvetica, ariel, &#39;sans serif&#39;">About Me</font></h2><font face="helvetica, ariel, &#39;sans serif&#39;">-->
            <!--and my supervisor is <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy Jinhua Ma</a>-->
            My research interests are in Multi-modality Learning and Data-centric AI. Currently, I focus on very large-scale efficient vision-language pre-training (<span style="color: #ff6600;">1,000 GPUs and 10 Billion Samples Level</span>) and Multi-modality Large Language Models. 
            I graduated from <a href="http://nxyc.nxeduyun.com">Ningxia Yucai High School</a>.
            I obtained my bachelor‚Äôs and master‚Äôs degrees from <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-Sen University (SYSU)</a>. 
            I completed my PhD at the <a href="https://nus.edu.sg">National University of Singapore (NUS)</a> <span style="color: #ff6600;">in three years</span>, where I was supervised by Prof. <a href="http://www.columbia.edu/~zs2262/">Mike Zheng Shou</a>. I am now a tenure-track faculty member in Central South University and have sufficient computational resources to support cutting-edge research.<br>
        <br><br>

        <div style="border: 2px solid #ff6600; padding: 10px; font-size: 14pt; text-align: center; background-color: #fff2cc;">
            <b> üéâüéâ I am looking for <span style="color: #20a0dc;">passionate and inquisitive </span>Masters, RAs, visiting students, and remote interns. </b><br>
            Research topics include <b>Multi-modality Large Language Model</b> and <b>Data-centric AI</b>.<br>
            Please drop me an email if interested: <a href="mailto:jinpengwang@u.nus.edu">link</a>.
        </div>

        <!-- <br><br> -->
        </p><hr size="2" align="left" noshade="">

        <h3>News   <img width="50" align="center" src="./imgs/news_gif2.gif" border="0"> &nbsp </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;"> 
            <b>[Feb 2025]</b> We release TextAtlas5M, a dataset specifically designed for training and evaluating multimodal generation models on dense-text image generation.
          </br>
            <b>[Oct 2024]</b> I will present my paper "Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning" in NeurIPS 2024 next month. See you in Vancouver! </br>
            <b>[Aug 2024]</b> Congratulation!! Our paper "Parrot Captions Teach CLIP to Spot Text" selected as an <b>oral </b> presentation in ECCV24. </br>
            <b>[June 2024]</b> We release VisInContext, an efficient Interleaved vision-language pre-training framework for increasing the in-context text length. <br>
            <b>[Jan 2024]</b> We release CosMo, <font color="#ff1493">an efficient Interleaved vision-language pre-training framework for both Image and Video!</font> We also release the first interleaved video-text dataset Howto-Interlink7M. See <a href="https://fingerrec.github.io/cosmo/"> Website </a> for more details. <br>
            <b>[Dec 2023]</b> We release "Parrot Captions Teach CLIP to Spot Text", an work focus on explore one simple bug in CLIP model. See <a href="https://linyq17.github.io/CLIP-Parrot-Bias/"> Website </a> for more details. <br>
            <b>[Dec 2023]</b> Our extension of "Position-guided Text Prompt for Vision Language Pre-training" is accepted by TPAMI.<br>
            <!-- <b>[May 2023]</b> I join in Microsoft Cloud&AI as a summer intern.<br> -->
            <b>[April 2023]</b> Our image2paragraph toolbox for fun, <a href="https://github.com/showlab/Image2Paragraph"> Code.</a> <br>
            <b>[Feb 2023]</b> Our PTP and All-in-one are all accepted by CVPR2023.<br>
            <b>[Dec 2022]</b> The work "Position-guided Text Prompt for Vision-language Pretraining" has been released in arxiv. Code and pretrained models available here: <a href="https://github.com/sail-sg/ptp"> Code.</a> <br>
        	<b>[Sep 2022]</b> Our first work for large-scale ego-centric pretrained method "Egocentric Video-Language Pretraining" (second-author) has been accepted by NeurIPS 2022. <br>
            <b>[July 2022]</b> One co-author work has been accepted by ECCV2022. <br>
            <b>[June 2022]</b> We won first place in CVPR's EPIC-Kitchens Challenge 2022 and OSCC of Ego4D Challenge. Looking <a href="https://qinghonglin.github.io"> kevin's website </a> for details. <br>
            <b>[Mar 2022]</b> We release the <font color="#ff1493">first and simplest e2e one-stream video-language pre-training </font> method: "All in One: Exploring Unified Video-Language Pre-training" in arix! Code and pretrained models are available here: <a href="https://github.com/showlab/all-in-one"> Code.</a><br> 
            <!--<b>[July 2021]</b> I'll join the Tencent PCG as a research internship these days.  <br>-->
            <b>[Mar 2022]</b> Congratulation!! Our paper "Object-aware Video-language Pre-training for Retrieval" has been accepted by CVPR2022! <a href="https://github.com/FingerRec/OA-Transformer"> Code.</a><br>
            <b>[Mar 2022]</b> Two previous co-author papers has been accepted by ICME. <br>
            <b>[Feb 2022]</b> One previous co-author paper "Hierarchical Feature Disentangling Network for Universal Domain Adaptation" has been accepted by Pattern Recognition (PR) (IF: 7.2). <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322000978">URL </a><br>
            <b>[Nov 2021]</b> Congratulation! Our paper "Suppressing Static Visual Cues via Normalizing Flows for Self-supervised Video Representation Learning" has been accepted by AAAI2022. It will be released in Arxiv soon. <br>
            <b>[July 2021]</b> Congratulation! Our paper "Learning Spatio-temporal Representation by Channel Aliasing Video Perception" has been accepted by ACM MM.  <br>
            <b>[July 2021]</b> I was awarded the excellent master graduation thesis (1/224).  <br>
            <b>[June 2021]</b> Our paper "Multi-level Temporal Dilated Dense Prediction for Action Recognition" has been published in IEEE Transactions on MultiMedia (TMM) (IF: 6.1). <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">URL </a> <br>
            <b>[Apr 2021]</b> Congratulation!!! I will take part in <a href="https://sites.google.com/view/showlab">Show Lab </a> at the August of this year. <br>
            <b>[Mar 2021]</b> I was invited to give a talk in VALSE-semanir in 3.28. Weclome to take part in us! <a href="https://mp.weixin.qq.com/s/b7ypH650J3v5pf_Qm7B22w">Link!</a><br>
                <b>[Mar 2021]</b> Our BE was recommended to CVer! <a href="https://mp.weixin.qq.com/s/HNVautdcOCOCqRllUhM7Tg">Link!</a>><br>
            <b>[Mar 2021]</b> I will serve as a reviewer for ICCV2021!<br>
            <b>[Mar 2021]</b> Our works on video self-supervised learning were accepted to CVPR!<br>
            <b>[Mar 2021]</b> Our work on self-supervised mutual learning was accepted to ICME!<br>
            <b>[Jan 2021]</b> I will serve as a reviewer for CVPR2021!<br>
            <b>[Oct 2020]</b> Our work on using contrastive learning for video action recognition was accepted to AAAI!<br>
            <b>[Aug 2020]</b> The code for our 3D Net Visualization has been relased in  <a href="https://github.com/FingerRec/3DNet_Visualization"> My Github</a>, support no-label visualization.<br>
            <b>[Apr 2020]</b> Our work on action recognition with dense prediction network was accepted to TCSVT!<br>
            <b>[Feb 2018]</b> The code for our real time action recognition has been relased in  <a href="https://github.com/FingerRec/real_time_video_action_recognition"> My Github</a>,
           </span>


        </p><hr size="2" align="left" noshade="">

        <h2>Publications ÔºàFirst/Co-first AuthorÔºâ </h2>
        <h3> <img width="50" align="center" src="./imgs/time_icon.png" border="0"> &nbsp 2025.1.1 - Now </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
        <tbody>
                
                <!-- <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2020</b></span>
                </td></tr>
                -->
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="100" align="center" src="index_files/jinpeng/papers/TextAtlas5M2025/intro_dataset-1.svg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>,  Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, Lijuan Wang, Min Li <br>
                        Arxiv, 2025. <br>
                        [<a href="https://arxiv.org/pdf/2502.07870">Paper</a>]
                        [<a href="https://github.com/CSU-JPG/TextAtlas">GitHub</a>]
                        [<a href="https://textatlas5m.github.io/">Website</a>]           
                        <br>
                    </td>
                </tr>
              </span>
            </span>
          </td>
        </tr>
        </tbody></table>

        <h3> <img width="50" align="center" src="./imgs/time_icon.png" border="0"> &nbsp 2023.1.1 - 2024.12.31 </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
                
                <!-- <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2020</b></span>
                </td></tr>
                -->
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="100" align="center" src="index_files/jinpeng/papers/ARXIV2024/motivation.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>, Linjie Li, Yiqi Lin, Min Li, Lijuan Wang and <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a> <br>
                        To appear in NeurIPS, 2024. <br>
                        [<a href="https://arxiv.org/abs/2406.02547">Paper</a>]
                        [<a href="https://github.com/showlab/visincontext">GitHub</a>]
                        [<a href="https://fingerrec.github.io/visincontext/">Website</a>]           
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="100" align="center" src="index_files/jinpeng/papers/ARXIV2023/resources/cosmo_motivation.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>COSMO: Contrastive Streamlined Multimodal Model With Interleaved Pre-Training.</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang and <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a> <br>
                        Arxiv, 2024. <br>
                        [<a href="https://arxiv.org/abs/2401.00849">Paper</a>]
                        [<a href="https://github.com/showlab/cosmo">GitHub</a>]
                        [<a href="https://fingerrec.github.io/cosmo/">COSMO Website</a>]
                        [<a href="https://fingerrec.github.io/cosmoe/">COSMOE Website</a>]
                        <!-- [<a href="./index_files/jinpeng/papers/ARXIV2023/resources/bibtex.txt">Bibtex</a>] -->
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="230" height="120" align="center" src="index_files/jinpeng/papers/ARXIV2023/resources/parrot_teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Parrot Captions Teach CLIP to Spot Text.</b> <br>
                        <span style="font-size: 10pt;">
                        Yiqi Lin, Conghui He, <b> Alex Jinpeng Wang (equal contribution) </b>, Bin Wang, Weijia Li, <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a> <br>
                        To appear in ECCV, 2024. (Oral) <br>
                        [<a href="https://arxiv.org/pdf/2312.14232.pdf">Paper</a>]
                        [<a href="https://github.com/opendatalab/CLIP-Parrot-Bias">GitHub</a>]
                        <!-- [<a href="./index_files/jinpeng/papers/ARXIV2023/resources/bibtex.txt">Bibtex</a>] -->
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="100" align="center" src="index_files/jinpeng/papers/ARXIV2023/resources/motivation-2.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Too Large; Data Reduction for Vision-Language Pre-Training.</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>, Kevin Qinghong Lin, David Junhao Zhang, Stan Weixian Lei and <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a> <br>
                        To appear in ICCV, 2023. <br>
                        [<a href="https://arxiv.org/pdf/2305.20087.pdf">Paper</a>]
                        [<a href="https://github.com/showlab/data-centric.vlp">GitHub</a>]
                        <!-- [<a href="./index_files/jinpeng/papers/ARXIV2023/resources/bibtex.txt">Bibtex</a>] -->
                        <br>
                    </td>
                </tr>
                </tbody></table>

        <h3> <img width="50" align="center" src="./imgs/time_icon.png" border="0"> &nbsp 2022.1.1 - 2022.12.31 </h3>

        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
                
                <!-- <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2020</b></span>
                </td></tr>
                -->

                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="100" align="center" src="index_files/jinpeng/papers/ARXIV2023/resources/motivation_2.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Position-guided Text Prompt for Vision Language Pre-training.</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>, <a href="https://scholar.google.com/citations?user=0b7ZqlcAAAAJ&hl=en"> Pan Zhou </a>, <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a>, <a href="https://scholar.google.com.hk/citations?user=DNuiPHwAAAAJ&hl=zh-CN">Shuicheng Yan</a> <br>
                        To appear in CVPR, 2023. <br>
                        [<a href="https://arxiv.org/abs/2212.09737">Paper</a>]
                        [<a href="https://github.com/sail-sg/ptp">GitHub</a>]
                        [<a href="./index_files/jinpeng/papers/ARXIV2023/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="100" align="center" src="index_files/jinpeng/papers/ARXIV2022/resources/introduction.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>All in One: Exploring Unified Video-Language Pre-training.</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=TtU74NAAAAAJ"> Yixiao Ge </a>, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a> <br>
                        To appear in CVPR, 2023. <br>
                        [<a href="https://arxiv.org/abs/2203.07303">Paper</a>]
                        [<a href="https://github.com/showlab/all-in-one">GitHub</a>]
                        [<a href="./index_files/jinpeng/papers/ARXIV2022/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="150" align="center" src="index_files/jinpeng/papers/CVPR2022/resources/1_motivation.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Object-aware Video-language Pre-training for Retrieval.</b> <br>
                        <span style="font-size: 10pt;">
                        <b> Alex Jinpeng Wang </b>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=TtU74NAAAAAJ"> Yixiao Ge </a>, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Zheng Shou</a> <br>
                        To appear in CVPR, 2022. <br>
                        [<a href="https://arxiv.org/abs/2112.00656">Paper</a>]
                        [<a href="./index_files/jinpeng/papers/CVPR2022/project_website.html">Webpage</a>]
                        [<a href="https://github.com/FingerRec/BE">GitHub</a>]
                        [<a href="./index_files/jinpeng/papers/CVPR2022/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="150" align="center" src="index_files/jinpeng/papers/AAAI2022/motivation.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Suppressing Static Visual Cues in Probability via Normalizing Flows for Self-Supervised Video Representation Learning.</b> <br>
                        <span style="font-size: 10pt;">
                        Manlin Zhang, <b>Jinpeng Wang (equal contribution)</b>, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a> <br>
                            To appear in AAAI, 2022. <b> Oral. </b><br>
                        [<a href="https://arxiv.org/abs/2112.03803">Paper</a>]
                        [<a href="./index_files/jinpeng/papers/AAAI2022/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                </tbody></table>
            <h3> <img width="50" align="center" src="./imgs/time_icon.png" border="0"> &nbsp 2021.1.1 - 2021.12.31 </h3>

                    <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="index_files/jinpeng/papers/ACMMM2021/resources/Intro.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Learning Spatio-temporal Representation by Channel Aliasing video Perception.</b> <br>
                        <span style="font-size: 10pt;">
                            Yiqi Lin, <b>Jinpeng Wang (equal contribution)</b>, Manlin  Zhang,  <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a> <br>
                        To appear in ACM MM, 2021. <br>
                        [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475394">Paper</a>]
                        [<a href="">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" height="90" align="center" src="index_files/jinpeng/papers/TMM2021/resources/mtp_illustration.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Multi-level Temporal Dilated Dense Prediction for Action Recognition</b> <br>
                        <span style="font-size: 10pt;">
                            <b>Jinpeng Wang</b>, Yiqi Lin, Manlin  Zhang,  Yuan Gao, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a> <br>
                        To appear in TMM, 2021. <br>
                        [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9447897">Paper</a>]
                        [<a href="./index_files/jinpeng/papers/TMM2021/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="index_files/jinpeng/papers/CVPR2021/resources/teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning</b> <br>
                        <span style="font-size: 10pt;">
                        <b>Jinpeng Wang</b>, Yuting Gao, Ke Li, Yiqi Lin, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a>, Hao Cheng, Pai Peng, <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji</a>, <a href="https://www.eee.hku.hk/~sunxing/"> Xing Sun</a>  <br>
                        To appear in CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2009.05769">Paper</a>]
                        [<a href="./index_files/jinpeng/papers/CVPR2021/project_website.html">Webpage</a>]
                        [<a href="https://www.youtube.com/embed/aFzno6CQcyE">Video</a>]
                        [<a href="https://github.com/FingerRec/BE">GitHub</a>]
                        [<a href="./index_files/jinpeng/papers/CVPR2021/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

<!--                <tr>-->
<!--                    <td width="30%" align=center>-->
<!--                    &lt;!&ndash; height="74" &ndash;&gt;-->
<!--                        &lt;!&ndash; <center> &ndash;&gt;-->
<!--                        <img width="260" height="180" align="center" src="index_files/jinpeng/papers/ICME2021/ICME_teaser.jpg" border="0"> &nbsp;-->
<!--                        &lt;!&ndash; </center> &ndash;&gt;-->
<!--                    </td>-->
<!--                    <td>-->
<!--                        <span style="font-size: 12pt;">-->
<!--                        <b>Self-supervised Mutual Learning For Video Representation Learning</b> <br>-->
<!--                        <span style="font-size: 10pt;">-->
<!--                            <b>Jinpeng Wang</b>, Yutong Li, Jianguo Hu, Xuebing Yang, Yanyu Ding  <br>-->
<!--                        To appear in ICME, 2021. <br>-->
<!--                        [<a href="">Paper</a>]-->
<!--                        [<a href="">Bibtex</a>]-->
<!--                        <br>-->
<!--                    </td>-->
<!--                </tr>-->
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                          <!-- <center> -->
                        <img width="260" align="center" src="./index_files/jinpeng/papers/AAAI2021/spatial_warpping.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion </b> <br>
                        <span style="font-size: 10pt;">
                            <span style="font-size: 10pt;"> <b> Jinpeng Wang </b>, Yuting Gao, Ke Li, Jianguo Hu, Xinyang Jiang, Xiaowei Guo, <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji</a>, <a href="https://www.eee.hku.hk/~sunxing/"> Xing Sun</a> <br>
                        In AAAI, 2021. <br>
                        [<a href="https://arxiv.org/abs/2009.05757">Paper</a>]
                        [<a href="./index_files/jinpeng/papers/AAAI2021/project_website.html">Webpage</a>]
                        [<a href="">Video</a>]
                        [<a href="https://github.com/FingerRec/DSM-decoupling-scene-motion">GitHub</a>]
                        [<a href="">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                            </tbody></table>
            <h3> <img width="50" align="center" src="./imgs/time_icon.png" border="0"> &nbsp 2020 & before </h3>

                    <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                          <!-- <center> -->
                        <img width="260" align="center" src="./index_files/jinpeng/papers/TCSVT2020/tcsvt_heatmap.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Revisiting Hard Example for Action Recognition </b> <br>
                        <span style="font-size: 10pt;">
                            <span style="font-size: 10pt;"> <b>Jinpeng Wang</b>, Jianguo Hu, Shiren Li, Zhihao Yuan <br>
                        In TCSVT, 2020. <br>
                        [<a href="https://ieeexplore.ieee.org/document/9026815">Paper</a>]
                        [<a href="https://github.com/FingerRec/RHE">GitHub</a>]
                        [<a href="">Bibtex</a>]
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                          <!-- <center> -->
                        <img width="260" align="center" src="./index_files/jinpeng/papers/ICASSP2020/background.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Rethinking Temporal-Related Sample for Human Action Recognition </b> <br>
                        <span style="font-size: 10pt;">
                        <span style="font-size: 10pt;">
                            <b>Jinpeng Wang</b>, Shiren Li, Zhikui Duan, Zhihao Yuan  <br>
                        In ICASSP, 2020. <br>
                        [<a href="https://ieeexplore.ieee.org/document/9053794">Paper</a>]
                        [<a href="">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                <tr>-->
<!--                    <td width="30%" align=left>-->
<!--                    &lt;!&ndash; height="74" &ndash;&gt;-->
<!--                        &lt;!&ndash; <center> &ndash;&gt;-->
<!--                          &lt;!&ndash; <center> &ndash;&gt;-->
<!--                        <img width="260" align="center" src="./index_files/jinpeng/papers/NeuroComputing2020/neurocomputing_teaser.png" border="0"> &nbsp;-->
<!--                        &lt;!&ndash; </center> &ndash;&gt;-->
<!--                    </td>-->
<!--                    <td>-->
<!--                        <span style="font-size: 12pt;">-->
<!--                        <b> Adversarial open set domain adaptation via progressive selection of transferable target samples </b> <br>-->
<!--                        <span style="font-size: 10pt;">-->
<!--                        <span style="font-size: 10pt;"> Yuan Gao, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a>, Yue Gao, Jinpeng Wang, Youngsun Pan <br>-->
<!--                        In NeuroComputing, 2020. <br>-->
<!--                        [<a href="https://www.sciencedirect.com/science/article/pii/S0925231220308675">Paper</a>]-->
<!--                        [<a href="">Bibtex</a>]-->
<!--                        <br>-->
<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td width="30%" align=left>-->
<!--                    &lt;!&ndash; height="74" &ndash;&gt;-->
<!--                        &lt;!&ndash; <center> &ndash;&gt;-->
<!--                          &lt;!&ndash; <center> &ndash;&gt;-->
<!--                        <img width="260" align="center" src="./index_files/jinpeng/papers/WACV2020/wacv_teaser.png" border="0"> &nbsp;-->
<!--                        &lt;!&ndash; </center> &ndash;&gt;-->
<!--                    </td>-->
<!--                    <td>-->
<!--                        <span style="font-size: 12pt;">-->
<!--                        <b> Multi-Scale Adversarial Cross-Domain Detection with Robust Discriminative Learning </b> <br>-->
<!--                        <span style="font-size: 10pt;">-->
<!--                        <span style="font-size: 10pt;"> Youngsun Pan, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a>, Yuan Gao, Jinpeng Wang, Yiqi Lin <br>-->
<!--                        In WACV, 2020. <br>-->
<!--                        [<a href="https://ieeexplore.ieee.org/document/9093287">Paper</a>]-->
<!--                        [<a href="">Bibtex</a>]-->
<!--                        <br>-->
<!--                    </td>-->
<!--                </tr>-->

<!--                <tr>-->
<!--                    <td width="30%" align=left>-->
<!--                    &lt;!&ndash; height="74" &ndash;&gt;-->
<!--                        &lt;!&ndash; <center> &ndash;&gt;-->
<!--                          &lt;!&ndash; <center> &ndash;&gt;-->
<!--                        <img width="260" align="center" src="./index_files/jinpeng/papers/ICIP2020/ICIP_teaser.png" border="0"> &nbsp;-->
<!--                        &lt;!&ndash; </center> &ndash;&gt;-->
<!--                    </td>-->
<!--                    <td>-->
<!--                        <span style="font-size: 12pt;">-->
<!--                        <b> Infrared-Visible Person Re-Identification Via Cross-Modality Batch Normalized Identity Embedding And Mutual Learning</b> <br>-->
<!--                        <span style="font-size: 10pt;">-->
<!--                        <span style="font-size: 10pt;"> Yiqi Lin, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a>, Jinpeng Wang <br>-->
<!--                        In ICIP, 2020. <br>-->
<!--                        [<a href="-">Paper</a>]-->
<!--                        [<a href="">GitHub</a>]-->
<!--                        [<a href="">Bibtex</a>]-->
<!--                        <br>-->
<!--                    </td>-->
<!--                </tr>-->

                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                          <!-- <center> -->
                        <img width="260" align="center" src="./index_files/jinpeng/papers/ICIG2019/icig_teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Spatio-Temporal Bottom-Up Top-Down Attention Model for Action Recognition </b> <br>
                        <span style="font-size: 10pt;">
                        <span style="font-size: 10pt;">
                            <b>Jinpeng Wang</b>, <a href="https://scholar.google.com.hk/citations?user=nhghITMAAAAJ&hl=en">Andy J. Ma</a> <br>
                        In ICIG, 2019. <br>
                        [<a href="https://link.springer.com/chapter/10.1007/978-3-030-34120-6_7">Paper</a>]
                        [<a href="">Bibtex</a>]
                        <br>
                    </td>
                </tr>
            </tbody></table>

        <!--<h2>Thesis </h2>-->
        <!--<font face="helvetica, ariel, &#39;sans serif&#39;">-->
            <!--<table cellspacing="15">-->
                <!--<tbody>-->
                <!--<tr>-->
                    <!--<td width="30%" align=left>-->
                        <!--<img width="250" align="center" src="./index_files/berkeley_logo.png" border="0">-->
                            <!--</td>-->
                    <!--<td>-->
                        <!--<span style="font-size: 12pt;">-->
                        <!--<b>Image Synthesis for Self-Supervised Visual Representation Learning</b> <br>-->
                        <!--<span style="font-size: 10pt;">-->
                        <!--Richard Zhang<br>-->
                        <!--&lt;!&ndash; Commitee: Alexei A. Efros, Trevor Darrell, Michael DeWeese.<br> &ndash;&gt;-->
                        <!--Spring 2018.<br>-->
                        <!--[<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-36.html">Thesis</a>]-->
                        <!--[<a href="https://www.youtube.com/watch?v=aGhYitrOJRc">Dissertation Talk</a>]-->
                        <!--[<a href="https://youtu.be/IXG-uWFAkmM">Fast Forward</a>]-->
                        <!--[<a href="https://www.dropbox.com/s/96f0xhfwvjnbf52/presentation_dissertation.pptx?dl=0">Slides</a> (396 MB)]-->
                        <!--[<a href="./index_files/bibtex_thesis.txt">Bibtex</a>]-->
                        <!--<br>-->
                    <!--</td>-->
                <!--</tr>-->
            <!--</tbody></table>-->

        <h2>Awards </h2>
        <span style="font-size: 10pt;">
        2022 Showlab Annual Award. ($1,000) <br>
        The champion of CVPR'22 Epic-kinetics challenge (2022). <br>
        The 1st place in Ego4D challenge (2022). <br>
        The final awarded list of AI SINGAPORE PhD FELLOWSHIP PROGRAMME ($$240,000). <br>
        2021 Excellent Graduation Thesis (1/224). <br>
        Reviewer recognitions, CVPR 2021, ICCV 2021, AAAI 2021, TCSVT.<br>
        The First Prize Scholarship (2020).<br>
        Second Prize of College Students Innovation and Entrepreneurship Competition (2018).<br>
        Excellent undergraduate thesis (2017).<br>
        </span>


<!--         <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="25">
                <tbody>
                <tr>
                    <td width="30%" align="center">
                        <img height="75" horizontal-align="center" src="./index_files/Adobe-logo.png" border="0">
                            </td>
                    <td>
                        <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017
                    </td>
                </tr>
            </tbody></table>
        </font> -->
        <br>

        <h2>Collaborators</h2>
        I have gotten to work with some wonderful collaborators.<br>

        <span style="font-size: 14pt;">
            <img width="80" align="center" src="imgs/Microsoft-Azure-Logo.png" border="0"> &nbsp;<b>@Microsoft Azure AI</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
            <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a>, Research Scientist <br>
            <a href="https://zyang-ur.github.io">Zhengyuan Yang</a>, Research Scientist <br>
            <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=zh-CN">Lijuan Wang</a>, Principal Research Manager<br>

        </dl>

        <span style="font-size: 14pt;">
               <img width="60" align="center" src="imgs/Sea_Group_logo.png" border="0"> &nbsp;<b>@Sea AI Lab</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
            <a href="https://panzhous.github.io">Pan Zhou</a>, @SAIL <br>
            <a href="https://yanshuicheng.ai">Shuicheng Yan</a>, Professor of @NUS<br>

        </dl>


        <span style="font-size: 14pt;">
               <img width="60" align="center" src="imgs/Tencent_arc.jpg" border="0"> &nbsp;<b>@Tencent PCG ARC Lab</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
            <a href="https://geyixiao.com">Yixiao Ge</a>, @CUHK <br>
            <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=zh-CN"> Ying Shan </a>, @Tencent <br>
        </dl>

        <span style="font-size: 14pt;">
               <img width="60" align="center" src="imgs/logo-youtu.png" border="0"> &nbsp;<b>@Tencent Youtu Lab</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
            <a href="https://www.sunxing.org">Xin Sun</a>, @HKU <br>
            <a href="https://mac.xmu.edu.cn/rrji_en/">Rongrong Ji</a>, Professor of @XiaMen University<br>

        </dl>

        <!-- </p> -->
        </span>
        <span style="font-size: 14pt;">
            <img width="60" align="center" src="imgs/Sun_Yat-sen_University_Logo.png" border="0"> <b>@Sun Yat-sen University</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
            <a href="http://www.cs.jhu.edu/~andyjhma/index.html">Andy Jinhua Ma</a>, Associate Professor <br>

        </dl>

        <!-- </p> -->
        </span>
        <!-- </p><hr size="2" align="left" noshade=""> -->


        <!--<h2>Teaching </h2>-->
        <!--&lt;!&ndash; <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017 &ndash;&gt;-->
        <!--<span style="font-size: 12pt;">-->
        <!--<b>Introduction to Artificial Intelligence (CS 188)</b>, UC Berkeley <br>-->
        <!--<span style="font-size: 10pt;">-->
        <!--Graduate Student Instructor (GSI) with Prof. <a href="http://people.eecs.berkeley.edu/~anca/">Anca Dragan</a> <br>-->
        <!--Spring 2017<br>-->
        <!--<br>-->
        <!--<span style="font-size: 12pt;">-->
        <!--<b>Computer Vision (CS 280)</b>, UC Berkeley <br>-->
        <!--<span style="font-size: 10pt;">-->
        <!--Graduate Student Instructor (GSI) with Prof. <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Prof. <a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> <br>-->
        <!--Spring 2016<br>-->
        <!--<br>-->
        <!--<span style="font-size: 12pt;">-->
        <!--<b>Introduction to Circuits (ECE 2100)</b>, Cornell University <br>-->
        <!--<span style="font-size: 10pt;">-->
        <!--Teaching Assistant (TA) with Prof. <a href="https://molnargroup.ece.cornell.edu/">Alyosha Molnar</a> <br>-->
        <!--Spring 2010<br>-->
        <!--<br>-->
            <!--<iframe src="//player.bilibili.com/player.html?aid=247300958&bvid=BV1tv41187Pk&cid=316899973&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>-->
        <h2>Talk</h2>
        <table border="1">
        <tr>
            <td>
            <b> Time:</b> 2021.2.12; <b> Title:</b> CVPR21 BE Demo; <b> Source:</b> Youtube
            <p align="center">
           <iframe width="430" height="300" src="https://www.youtube.com/embed/aFzno6CQcyE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
              </p>
            </td>
            <td>
                <b> Time:</b> 2021.3.29;  <b> Title:</b> VALSE STUDENT WEBINAR;  <b> Source:</b> Bilibili (China)
            <p align="center">
<!--             <iframe width="430" height="300" src="//player.bilibili.com/player.html?aid=247300958&bvid=BV1tv41187Pk&cid=316899973&page=1" sandbox frameborder="0" allow="encrypted-media" allowfullscreen align="center" ></iframe> -->
            <iframe width="430" height="300" src="//www.bilibili.com/blackboard/html5mobileplayer.html?aid=247300958&bvid=BV1tv41187Pk&cid=316899973&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
              <!-- <video width="430" height="300" controls>
                <source src="//player.bilibili.com/player.html?aid=247300958&bvid=BV1tv41187Pk&cid=316899973&page=1" type="video/mp4"> -->
            </video>
            </p>
            </td>
        </tr>
        </table>


        <h2>Visitors</h2>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=YbPKF1QuCrlgXX8mwopH6mb4xLtY9uixfzcI2QNBsJA'></script>

            <h2>More information</h2>
        Confused by the contents of this page? You can contact with my email.
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75897335-1', 'auto');
  ga('send', 'pageview');
</script>

</font></td></tr></tbody></table><iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div></body></html>

